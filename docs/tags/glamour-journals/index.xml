<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Glamour journals on Claus O. Wilke</title>
    <link>/tags/Glamour-journals/</link>
    <description>Recent content in Glamour journals on Claus O. Wilke</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 04 Jan 2014 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/tags/Glamour-journals/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How glamour journals rose to prominence, and why they may not be needed anymore</title>
      <link>/blog/2014/01/04/how-glamour-journals-rose-to-prominence-and-why-they-may-not-be-needed-anymore/</link>
      <pubDate>Sat, 04 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/blog/2014/01/04/how-glamour-journals-rose-to-prominence-and-why-they-may-not-be-needed-anymore/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In the ongoing discussion about the value of glamour journals such as Nature, Science, and Cell, I think it’s worth looking back and asking: “How did they rise to prominence?” and “Are they still serving the same role they did when they arose?” So let’s take a quick trip into the history of science communication, before the internet. Then we can ask what the internet has changed, and how we could make the best of current technology.&lt;/p&gt;
&lt;p&gt;I belong to the last generation of scientists that experienced science before the internet. I started doing research as an undergrad in 1995. At that time, I saw a web browser for the first time, and I sent my first email. While the internet had been around for a while by 1995,&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; its use was still very limited, and barely anybody outside academia had ever even heard of it. All this would change over the next 4-5 years, and by 2000 the internet started to become ubiquitous.&lt;/p&gt;
&lt;p&gt;I don’t think anybody who got into science after the year 2000 can imagine what keeping up with the literature was like before the internet. I started my postdoc in 2000. During my entire postdoc time (or since), I rarely ever went into a library. By that time, most journals had a solid online presence, including back issues. Articles that weren’t available online could be requested via inter-library loan, and they would arrive electronically. By contrast, during my PhD, I spent a lot of time at the library. I would make weekly trips to browse the latest issues of the scientific journals I was interested in. Because I was working at the interface between physics and biology, I had to visit the physics library and the biology library. For certain articles I also had to go to the chemistry library. I knew exactly which library had which journals, and which journals were available in multiple libraries. (Almost all libraries had Science, for example.) To figure out whether anybody was citing a particular paper, I had to confer with the &lt;a href=&#34;http://en.wikipedia.org/wiki/Science_Citation_Index&#34;&gt;Science Citation Index,&lt;/a&gt; which was a big book available at some libraries. For any article of interest, it would list by which other articles it had recently been cited. Invariably, the list of citing articles would contain articles in journals that were only available in a different library on campus, or not in any library at my university. So, after reading the Science Citation Index, I’d make the trip to a different library, or submit an inter-library loan request, or make a note for my next scheduled trip to a different library to look up a particular article. In the worst-case scenario, it could take weeks until I saw a particular article, and then I’d often find out the article wasn’t really relevant to what I was doing.&lt;/p&gt;
&lt;p&gt;Compare this to how literature search works today. I look up an article on Google Scholar, click on “Cited by” or “Related articles,” and find relevant related articles in seconds. For every article listed, I can get at least the title and the abstract, and for the vast majority of articles I can get the full text, all within a few seconds and while sitting at home on my couch. I can similarly browse any open-access journal, everything on Pubmed Central, and any paywalled journal my university has access to, from everywhere in the world. For all intents and purposes, the second I know a particular article exists, I can look it up and read it.&lt;/p&gt;
&lt;p&gt;What has any of this to do with glamour journals? I’m going to argue that in the time before the internet, glamour journals and other highly selective journals served a crucial role. In a world where looking up a reference can take between days and weeks, finding potentially interesting &lt;em&gt;references&lt;/em&gt; is much less valuable than finding potentially interesting &lt;em&gt;articles.&lt;/em&gt; Yes, you would go to the trouble of hunting down a particular article if it seemed directly relevant for your own work, but you certainly wouldn’t do so just to generally keep up with a broader field, much less all of science. Therefore, reading a journal such as Science or Nature, or even a more specialized but still fairly selective journal such as Genetics, was the only way to keep up with scientific progress. You went to the library, you took the physical copy of the journal, you browsed through it, you read the interesting articles, you learned something useful, you went home/back to your office.&lt;/p&gt;
&lt;p&gt;By contrast, these days, with nearly any article right at our fingertips, the process of publishing articles and of selecting articles can be decoupled. For example, I don’t consistently browse through the tables of contents of Nature or Science anymore, because I now have other means of discovering interesting articles. Any interesting article in my field I’ll come across sooner or later because Google Scholar recommends it to me, or somebody tells me about it in person, or it gets cited in a paper I read or review. For generally interesting articles, say the latest findings about global warming, I’ll likely see them mentioned on Twitter or Reddit. The advantage of all these methods of finding articles over browsing through tables of contents is that I’m not tied to the venue in which the article was published. I’m just as likely to come across an interesting article published in Nature as one in PLOS ONE. And the moment I learn about an article, I can read it. But imagine a print version of Twitter, in the time before the internet. If I had received per mail, once a week, a list of potentially interesting things published in the most random venues, I would never have followed up with reading any of them. The barrier to doing so would simply have been too high.&lt;/p&gt;
&lt;p&gt;Some people take this reasoning to the extreme and argue that since now everything is easily available online and search engines are powerful, we don’t need selective journals anymore at all. The best science will rise to the top, it will be cited, tweeted, mentioned on reddit, bloggers will write posts about it, and thus we might as well publish everything in PLOS ONE. I am not entirely convinced by this argument, for the following reason: It’s all well and good if other people cite and tweet your work, but what if they don’t? If you think you have done some really outstanding work, work that deserves more attention than your regular bread-and-butter efforts do, in a world where all science is published in PLOS ONE, what options to you have? In a world that has glamour journals, it’s of course obvious what you can do: You write a nice 3-6 page summary of your work, highlighting the most important findings and the broad relevance, you send it to one or more glamour journals, and you hope for the best. If you get through, you’ll have a much higher chance of getting your article cited, tweeted, etc., because people pay attention to the glamour journals, and they like to read short, clearly written articles that highlight key findings and broader relevance. But if there are no glamour journals, then you have no good option of indicating that in your own opionion, this article is more valuable than that article.&lt;/p&gt;
&lt;p&gt;So let me summarize the facts: (i) In the world of the internet, it doesn’t matter where something is physically published, as long as it is easily accessible through a URL. (ii) Glamour journals have lost the original purpose of making important science easily and broadly accessible. (iii) Publication venues that highlight interesting work by commenting and/or linking to it (such as Twitter, Reddit, Nature News and Views, Google Scholar, etc.) are highly valuable. (iv) Short, clearly written articles highlighting key insights and broader relevance are appreciated and highly valuable. (v) Authors have an interest in pointing out what they think are their most important works.&lt;/p&gt;
&lt;p&gt;These facts lead me to the following proposal: Let’s take all original science out of the glamour journals. Instead, allow authors to submit short summaries (maybe 2-3 pages) of work they have already published elsewhere, e.g. in PLOS ONE. These summaries would be reviewed editorially, and also by one or two expert scientists who’d be asked to judge whether the original article appears to be scientifically sound and noteworthy. Editors might reject a summary because it isn’t deemed sufficiently interesting or novel, and there would still be fighting and politicing about getting summaries into these glamour journals, but the system would relieve authors of several pressures: (i) Authors wouldn’t have to rewrite an article multiple times just to hope to get it published eventually in one of the selective journals. (ii) Authors could get their results out and cite them properly while still trying for that glamour slot. (iii) Since the original article of record would be in PLOS ONE or PeerJ or similar, it would become generally accepted to have even the most important work published in these journals. Nobody could look at a publication list and say “Oh, it’s just a bunch of PLOS ONE papers.” (iv) Hiring committees and granting agencies would still have the option to evaluate candidates by the number of summaries they have published in glamour journals, though I would hope they would do so to a lesser degree and pay more attention to the original articles.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The first web browser, which started the development of the modern internet, &lt;a href=&#34;http://en.wikipedia.org/wiki/Mosaic_web_browser&#34;&gt;was released in 1993.&lt;/a&gt; The concept of an online journal was unthinkable before the invention of the web browser.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Is there an avalanche of low-quality research, and if so, must we stop it?</title>
      <link>/blog/2013/12/21/is-there-an-avalanche-of-low-quality-research-and-if-so-must-we-stop-it/</link>
      <pubDate>Sat, 21 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>/blog/2013/12/21/is-there-an-avalanche-of-low-quality-research-and-if-so-must-we-stop-it/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; &lt;em&gt;It turns out the article in the Chronicle is not recent, I misread the date on the page. (The Chronicle has two dates on each page, today’s date and the article publication date.) I stand by everything else I say, though.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A recent article in the Chronicle of Higher Education argues that &lt;a href=&#34;http://chronicle.com/article/We-Must-Stop-the-Avalanche-of/65890/&#34;&gt;“we must stop the avalanche of low-quality research.”&lt;/a&gt; The authors decry the rapid growth of the scientific literature, which (as they argue) puts increasing strain on readers, reviewers, and editors without producing much benefit. They argue that this growth is driven by an increasing pressure on scientists to publish more, and the result is increasing amounts of low-quality publications. To address the pressure on scientists, they propose three fixes, of which one is Ok and two are positively inane. Maybe what we really have to stop is the avalanche of low-quality, non-reviewed opinion pieces published on web pages?&lt;/p&gt;
&lt;p&gt;Reading through the article, I found it difficult not to wonder whether the authors had ever heard of the internet or of modern information-processing technology (e.g., Google). Now, to be fair, none of the authors are in the natural sciences. The authors work in English, mechanical engineering, medicine, management, and geography. I don’t really know these areas. My own work is in biology, and I’m also somewhat familiar with the publishing cultures in physics and in computer science. So, everything they say may make sense in their fields, but it doesn’t in mine. I’m not convinced we have a major crisis, and I certainly don’t think their proposed fixes are any good. Let’s take a look at their proposed solutions first.&lt;/p&gt;
&lt;div id=&#34;limit-number-of-papers-submitted-for-job-applications-or-promotions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Limit number of papers submitted for job applications or promotions&lt;/h2&gt;
&lt;p&gt;The first fix, to limit the number of papers that applicants are allowed to submit for job applications or promotions, is actually somewhat reasonable. Applicants should be judged on the quality of their work, not on the mere quantity of output. However, I am strongly opposed to saying applicants are not even allowed to mention anything beyond their key 3-5 papers. Why should productivity be punished? What if they wrote 10 important papers? Should Ed Witten be limited to list only 5 papers? (To date, he has written &lt;a href=&#34;http://scholar.google.com/scholar?hl=en&amp;amp;q=edward+witten&#34;&gt;over 30 papers with over 1000 citations each!&lt;/a&gt;) While there are negative outliers in academia, people who produce huge amounts of mindless drivel, I definitely see a correlation between quantity and quality. The most interesting and influential papers are generally written by the most productive researchers. I have previously given arguments for why we would &lt;a href=&#34;/blog/2013/11/3/no-one-reads-your-paper-either&#34;&gt;expect such a correlation to exist.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Most job search and promotion processes that I am aware of have already found a solution to this problem, by asking applicants to submit both (i) a full list of all publications and (ii) the 3-5 most important papers, possibly with a statement explaining their impact. This is good practice that strikes a balance between quality and quantity, it allows applicants to showcase both how good they are and how consistently productive they are, and most importantly, it is already common practice. So point 1 is a non-issue, from where I stand.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluate-researchers-by-impact-factors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Evaluate researchers by impact factors&lt;/h2&gt;
&lt;p&gt;Evaluating researchers by impact factor is such an absurd and untimely suggestion, I can’t help but wonder whether the authors have been living under a rock for the last 10 years. It’s particularly ironic that the Chronicle of Higher Education would publish this statement a mere 11 days after nobel-prize winner Randy Schekman publicly proclaimed that luxury (i.e., high impact-factor) journals such as Nature, Cell, and Science &lt;a href=&#34;http://www.theguardian.com/commentisfree/2013/dec/09/how-journals-nature-science-cell-damage-science&#34;&gt;“are damaging science.”&lt;/a&gt; Did the authors really not see this article, &lt;a href=&#34;http://scholarlykitchen.sspnet.org/2013/12/11/this-takes-the-prize-editor-of-new-luxury-oa-journal-boycotts-luxury-subscription-journals/&#34;&gt;nor the widespread outrage it caused over containing a cheap plug for a different luxury journal?&lt;/a&gt; If there is one problem we have in science right now, at least in the biomedical field, it’s an over-reliance on impact factors and publications in high-profile journals. The outcry over Schekman’s article shows how sensitive of an issue this is, and how many scientists are concerned about the growing pressure to publish in only the highest-impact journals. Schekman himself addresses this in &lt;a href=&#34;http://theconversation.com/how-to-break-free-from-the-stifling-grip-of-luxury-journals-21669&#34;&gt;his response to the criticism he received.&lt;/a&gt; Scientists should be judged on the quality of their work, not on whether or not they published in Nature.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;limit-the-length-of-papers-published&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Limit the length of papers published&lt;/h2&gt;
&lt;p&gt;I don’t see how imposing page limits connects at all to the issue at hand. Surely, if we want fewer but higher-quality publications, the papers should be longer not shorter. Also, I strongly oppose to the split model with a brief (4-6 page) main article (i.e., advertisement) accompanied by longer supporting materials. Invariably, the supporting materials are not written as carefully as the main article, and the quality of the paper as a whole suffers. Notably, PNAS just went the other direction, and now allows papers of up to 10 pages in length in their online-only PNAS Plus edition. This was a very welcome change, I think. The 6-page limit of PNAS was often too limiting, whereas most articles fit comfortably within 10 pages.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;is-there-too-much-pressure-to-publish&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Is there too much pressure to publish?&lt;/h2&gt;
&lt;p&gt;While there is pressure to publish, frankly I don’t see that there is &lt;em&gt;excessive&lt;/em&gt; pressure to publish. From what I see, for example in conversations with colleagues, the common expectation is reasonable productivity both in terms of quantity and in terms of quality. In terms of quantity, reasonable is usually a number between 1 and 10 papers per year. Publish less, and people start wondering whether you’re working consistently, and in particular whether you’ll keep working in the future. Publish much more than 10 papers per year, and people start looking at you suspiciously. I sat on a grant-review panel once where one applicant claimed his previous 3-year NSF grant had led to ~100 publications. People were very suspicious of this claim and the grant did not get good reviews, even though the science seemed to be reasonable. (I’m not saying the proposal would have been funded if the applicant had had fewer publications, but the high number certainly didn’t help; if it had any effect it was a negative one.) In most areas of Biology, I think 2-3 papers a year will be considered perfectly reasonable for anybody but a senior PI running a large lab. (This includes all papers with your name on, not just first-author papers.)&lt;/p&gt;
&lt;p&gt;In terms of quality, I stick to my earlier recommendation: &lt;a href=&#34;/blog/2013/11/3/no-one-reads-your-paper-either&#34;&gt;publish at least one paper a year that has some real substance.&lt;/a&gt; Where exactly that paper is published is secondary, I believe. Publishing the occasional high-profile article in a luxury journal can’t hurt, but I hope that we as scientists can collectively learn to pay a little less attention to where something is published and pay more attention to the content. We shouldn’t hire somebody without having carefully read at least one or two of their papers, and I think the more diligent search committees operate like that already.&lt;/p&gt;
&lt;p&gt;With regards to excessive workload for editors and reviewers, I think there are several things that could be done relatively easily:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Institute a system of reviewing credits, where you receive one credit for each article you review and you have to spend a number of credits (e.g. 6) to submit an article. This would ensure that everybody who publishes carries their fair share on the reviewing side.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Have more graduate students and postdocs review papers. Not every paper needs to be reviewed by three members of the NAS. In fact, I often find that graduate students write better reviews than senior scientists do, because the graduate students take the job much more seriously and put way more effort into it than an established scientist normally would.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Have less stringent reviewing criteria, don’t judge impact. Much of the excessive reviewing load actually comes from the pressure to publish in highly selective journals. Thus, many articles make the mandatory trek from Science to Nature to PNAS to PLOS Genetics to PLOS ONE, possibly undergoing four or more separate rounds of review. It’s not uncommon for me to review the same article several times for different journals. And in the end, everything gets published anyway, somewhere. If it was the reviewers’ job to only look for major scientific flaws, then most articles could be published after 1-2 rounds of review, cutting the total review burden way down.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Improve tools for post-publication evaluation of articles. At present, all we have is citations and word-of-mouth. (“Have you seen the latest paper by X in PNAS? It’s really not very good.”) I’m sure we can do better than that, and over time we’ll find ways to put modern computing power and crowd-sourcing ideas to good use. &lt;a href=&#34;http://www.the-scientist.com/?articles.view/articleNo/37969/title/Post-Publication-Peer-Review-Mainstreamed/&#34;&gt;NCBI’s PubMed Commons is a first step in this direction.&lt;/a&gt; I’m sure over the next 10-20 years we’ll see many more innovative ideas to evaluate the quality of scientific work post publication.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The value of pre-publication peer review</title>
      <link>/blog/2013/12/21/the-value-of-pre-publication-peer-review/</link>
      <pubDate>Sat, 21 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>/blog/2013/12/21/the-value-of-pre-publication-peer-review/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I see lot of discussion these days about the value of peer review. Are journals too selective? Are acceptance decisions arbitrary? Does peer review actually catch scientific mistakes or fraudulent practices? Wouldn’t it be better to just put everything out there, say on preprint servers, and separate the wheat from the chaff in post-publication review? I’m not quite ready yet to give up on pre-publication peer review. I think it serves a useful purpose, one I wouldn’t want to do away with. In the following, I discuss four distinct services that peer review provides, and assess the value I personally assign to each of them.&lt;/p&gt;
&lt;div id=&#34;peer-review-screens-out-nonsense-and-pseudoscience&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Peer review screens out nonsense and pseudoscience&lt;/h2&gt;
&lt;p&gt;It’s important that somebody screen all potential scientific publications for actual scientific content. I don’t mind publishing null results, replication studies, or studies that present only a very minor advance. All of these works contain real science, and they may find some use at some point in the future. However, we must never mix science with pseudoscience. Someone has to assure that whatever gets published in a scientific journal is not complete nonsense. Even the preprint archive arxiv.org has &lt;a href=&#34;http://arxiv.org/help/endorsement&#34;&gt;some sort of a screening and filtering system in place to hold back the crackpots.&lt;/a&gt; In most cases, nonsensical papers would be caught by the editor and not even sent out to review. Nevertheless, we can consider filtering out nonsense to be an essential service of the pre-publication review process.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;peer-review-catches-major-mistakes-andor-fraud&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Peer review catches major mistakes and/or fraud&lt;/h2&gt;
&lt;p&gt;Many people seem to think that it is the reviewers’ job to catch major mistakes and/or fraud. And when they fail to do so, that is taken as evidence that peer review doesn’t work. I don’t think we can put such a high burden on the reviewers. Ultimately, the burden of producing correct and genuine results lies with the author. Peer review operates under the assumption that fundamentally the authors are honest and reasonably capable scientists. If peer review does happen to catch a major issue with a paper, that’s great, but generally I think that post-publication review is the much better venue to address major flaws or scientific misconduct.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;peer-review-assess-novelty-potential-impact-and-fit-with-the-journal-scope&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Peer review assess novelty, potential impact, and fit with the journal scope&lt;/h2&gt;
&lt;p&gt;Whether reviewers (or editors) should consider novelty and impact, and whether journals should be selective at all, is probably the most contentious issue in peer review. Traditionally, this has always been part of peer review. However, there are now several journals that explicitly state review should only assess scientific soundness (e.g. &lt;a href=&#34;http://www.plosone.org/static/information&#34;&gt;PLOS ONE&lt;/a&gt; or &lt;a href=&#34;https://peerj.com/about/aims-and-scope/&#34;&gt;PeerJ&lt;/a&gt;). I think there are valid arguments for both sides. On the one hand, it is imperative that we have publishing venues that will publish any scientifically sound study. Nobody benefits if a valid study is suppressed just because some reviewers didn’t find it interesting. If there’s no obvious scientific flaw, put it out there and let the readers (and Google) sort it out.&lt;/p&gt;
&lt;p&gt;On the other hand, I think that more selective journals can provide value as well. In my mind, where science has gone off-track is that the most selective journals (which are also considered to be the most prestigious ones, e.g. Nature, Science, Cell, PLOS Biology, PNAS) employ arbitrary selection criteria based primarily on the subjective goal of publishing “the best science.” As a consequence, whether I can publish in such journals depends much more on my marketing skills than on my scientific skills, and also on whether I’m working on a sexy study system.&lt;/p&gt;
&lt;p&gt;By contrast, the next lower tier of selective journals usually employ more objective selection criteria, and those arguably provide a useful value. For example, I’m an Associate Editor for PLOS Computational Biology, a fairly selective journal. The main requirement for publication in PLOS Computational Biology is &lt;a href=&#34;http://www.ploscompbiol.org/static/information&#34;&gt;that you have produced high quality computational work that yields a novel biological insight.&lt;/a&gt; In my mind, it is fairly straightforward to determine whether a paper satisfies that requirement or not. I also think that any capable computational biologist can jump over that bar. As a consequence, I feel that we’re providing useful selectivity without generating excessive artificial scarcity or making highly arbitrary decisions. If I see that somebody has on their CV a couple of PLOS Computational Biology papers, I can reasonably assume that they are doing consistent, high-quality computational work leading to novel insights into biological systems.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;peer-review-helps-authors-improve-their-articles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Peer review helps authors improve their articles&lt;/h2&gt;
&lt;p&gt;In my mind, this last point is the most important point, and the reason why I’m not willing to give up pre-publication review in its entirety. In my experience as author, reviewer, and editor, the most common outcome of the review process other than “reject due to insufficient novelty” is “major revision.” The reviewers agree that the study has merit in principle, but they see a number of possible revisions that would improve the article. I have seen it countless times, both as author and as reviewer or editor, that a study was vastly improved after the first set of reviews. Sometimes reviewers catch an issue the authors hadn’t noticed, sometimes they have a really cool idea that brings the study to the next level, and sometimes they simply tell you that you have to work on your writing if you want to get your point across. Either way, this input is invaluable, and it improves the scientific literature tremendously. If we went to a system that operated entirely on post-publication review, we would probably still see the same kind of comments by reviewers, but there would be very little incentive for the authors to go and revise their papers accordingly.&lt;/p&gt;
&lt;p&gt;One downside to this aspect of peer review is that sometimes reviewers just keep insisting on changes that the authors don’t deem necessary or appropriate. This is another form of peer review gone wrong. The reviewers should make helpful suggestions, but they should not tell the authors how to write their paper. One solution to this issue is to make peer reviews public and leave with the authors the ultimate decision of whether or not they want to publish, as &lt;a href=&#34;http://www.biologydirect.com/about&#34;&gt;Biology Direct does.&lt;/a&gt; Another possibility is to allow authors to opt-out of re-review, as &lt;a href=&#34;http://www.biomedcentral.com/bmcbiol/about#publication&#34;&gt;BMC Biology does.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
